"""MrZeroExploitBuilder - LLM-Powered Weaponizer Agent."""

import asyncio
import json
import re
import shutil
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any

from mrzero.agents.base import AgentResult, AgentType, BaseAgent
from mrzero.core.memory.state import AgentState
from mrzero.core.schemas import (
    Exploit,
    EnvironmentInfo,
    Vulnerability,
    VulnerabilityType,
)


class ExploitBuilderAgent(BaseAgent):
    """Agent for building exploits using LLM-driven dynamic analysis.

    This agent uses LLM reasoning as the PRIMARY decision maker for:
    1. Analyzing vulnerabilities and determining exploitation strategy
    2. Generating targeted exploit code (not generic templates)
    3. Using dynamic analysis tools (debuggers, fuzzers) to refine exploits
    4. Iterating based on test results until exploitation succeeds

    The LLM is the brain - tools (pwntools, ROPgadget, GDB, Frida) are its hands.
    """

    agent_type = AgentType.EXPLOIT_BUILDER

    SYSTEM_PROMPT = """You are MrZeroExploitBuilder, an elite exploit developer and security researcher specializing in vulnerability weaponization.

## Your Mission
Convert confirmed vulnerabilities into working exploits. You don't use generic templates - you craft TARGETED exploits based on deep analysis of the vulnerability, the target environment, and the specific code paths involved.

## Your Expertise

### Web Application Exploitation
- SQL Injection: Union-based, blind (boolean/time), error-based, stacked queries, out-of-band
- Command Injection: Shell metacharacters, argument injection, environment variable manipulation
- XSS: DOM-based, reflected, stored, CSP bypass, filter evasion
- SSRF: Protocol smuggling, DNS rebinding, cloud metadata exploitation
- Deserialization: Java (ysoserial), PHP, Python pickle, .NET, Ruby
- Path Traversal: Encoding bypass, null byte injection, symlink attacks
- Authentication Bypass: JWT attacks, session fixation, credential stuffing logic

### Binary Exploitation
- Buffer Overflow: Stack-based, heap-based, format strings
- ROP Chain Construction: Finding gadgets, chaining for shellcode execution
- Return-to-libc: Bypassing NX/DEP
- ASLR Bypass: Information leaks, partial overwrites, brute force
- Canary Bypass: Fork-based brute force, information leaks
- Heap Exploitation: Use-after-free, double-free, heap spray

### Smart Contract Exploitation
- Reentrancy: Single-function, cross-function, cross-contract
- Flash Loan Attacks: Price manipulation, arbitrage
- Integer Overflow/Underflow
- Access Control Bypass
- Front-running attacks

## Your Toolkit (Use These!)

### Dynamic Analysis
- **pwntools**: Python framework for CTF/exploit development
- **ROPgadget**: Find ROP gadgets in binaries
- **GDB/pwndbg**: Linux debugging and memory analysis
- **Frida**: Dynamic instrumentation for runtime analysis
- **strace/ltrace**: System/library call tracing

### Fuzzing
- **AFL++**: Coverage-guided fuzzing
- **Radamsa**: General-purpose fuzzer
- **Custom fuzzers**: Targeted mutation

### Web Tools
- **curl/httpie**: HTTP requests
- **sqlmap**: SQL injection automation
- **Burp Suite patterns**: Request manipulation

### Reverse Engineering
- **objdump/readelf**: Binary analysis
- **strings**: Extract strings from binaries
- **checksec**: Security mitigations check

## Exploitation Methodology

### Phase 1: Deep Vulnerability Analysis
- Understand the EXACT vulnerable code path
- Identify what input triggers the vulnerability
- Determine what constraints exist (filters, WAFs, sanitization attempts)
- Map the data flow from input to vulnerable sink

### Phase 2: Environment Analysis
- What protections are in place? (ASLR, NX, Canaries, PIE, RELRO)
- What's the target architecture? (x86, x64, ARM)
- What libraries are available for exploitation?
- What's the network configuration?

### Phase 3: Exploit Strategy
- Choose the right exploitation technique
- Plan payload delivery mechanism
- Design shellcode or post-exploitation actions
- Consider reliability and stealth

### Phase 4: Exploit Development
- Write targeted exploit code (NOT generic templates)
- Include error handling and edge cases
- Add debugging output for development
- Document the exploitation steps

### Phase 5: Testing & Iteration
- Test against the target environment
- Analyze failures and adjust
- Refine payloads based on results
- Verify successful exploitation

## Important Guidelines

- **TARGETED, NOT GENERIC**: Every exploit should be crafted for the specific vulnerability
- **USE TOOLS**: Leverage pwntools, ROPgadget, debuggers - don't reinvent the wheel
- **ITERATE**: If an exploit fails, analyze why and improve
- **DOCUMENT**: Include comments explaining the exploitation logic
- **SAFETY**: Include checks to prevent unintended damage"""

    EXPLOIT_STRATEGY_PROMPT = """## Task: Design Exploitation Strategy

Analyze the vulnerability and environment to create a targeted exploitation plan.

---

## Vulnerability Details:

{vulnerability_info}

---

## Target Environment:

{environment_info}

---

## Available Tools:

{available_tools}

---

## Your Task

Design a TARGETED exploitation strategy for this specific vulnerability.

Think step by step:
1. What is the exact vulnerability and how is it triggered?
2. What constraints or protections must be bypassed?
3. What exploitation technique is most appropriate?
4. What tools will you use?
5. What should the exploit achieve? (PoC crash, data exfil, RCE, etc.)

Respond in this exact JSON format:
```json
{{
    "vulnerability_analysis": {{
        "vuln_type": "<specific vulnerability type>",
        "trigger_mechanism": "<how to trigger the vulnerability>",
        "input_vector": "<where malicious input enters>",
        "vulnerable_function": "<specific function/endpoint>",
        "constraints": ["<constraint 1>", "<constraint 2>"],
        "protections_to_bypass": ["<protection 1>", "<protection 2>"]
    }},
    "exploitation_strategy": {{
        "technique": "<chosen exploitation technique>",
        "reasoning": "<why this technique>",
        "goal": "poc_crash" | "data_exfiltration" | "code_execution" | "privilege_escalation",
        "payload_type": "<type of payload needed>",
        "delivery_method": "<how payload will be delivered>"
    }},
    "tool_plan": [
        {{
            "tool": "<tool name>",
            "purpose": "<what we'll use it for>",
            "command_template": "<example command>"
        }}
    ],
    "exploit_steps": [
        {{
            "step": 1,
            "action": "<what to do>",
            "expected_result": "<what success looks like>",
            "fallback": "<what to try if this fails>"
        }}
    ],
    "success_indicators": ["<how we know exploitation succeeded>"],
    "estimated_complexity": "low" | "medium" | "high",
    "requires_interaction": <true|false>
}}
```"""

    EXPLOIT_GENERATION_PROMPT = """## Task: Generate Exploit Code

Based on the exploitation strategy, generate working exploit code.

---

## Vulnerability:

{vulnerability_info}

---

## Exploitation Strategy:

{strategy}

---

## Environment Details:

{environment_info}

---

## Binary Analysis (if applicable):

{binary_analysis}

---

## Your Task

Generate a COMPLETE, WORKING exploit. Not a template - actual exploit code that targets this specific vulnerability.

Requirements:
1. Include all necessary imports
2. Add detailed comments explaining each step
3. Include error handling
4. Add debugging output
5. Make it configurable (target URL/IP, ports, etc.)
6. Include a test/verification function

For binary exploits, use pwntools patterns:
```python
from pwn import *

# Set context
context.binary = './vulnerable_binary'
context.arch = 'amd64'
context.os = 'linux'

# Build payload with pwntools
payload = flat([
    b'A' * offset,
    p64(gadget_address),
    # ... ROP chain
])
```

For web exploits, be specific:
```python
import requests

def exploit(target_url):
    # Specific payload crafted for THIS vulnerability
    payload = "..."  # Not generic - targeted!
    
    response = requests.post(
        f"{{target_url}}/vulnerable/endpoint",
        data={{"param": payload}}
    )
    
    return check_success(response)
```

Respond in this exact JSON format:
```json
{{
    "exploit_code": {{
        "language": "<python|c|solidity|javascript>",
        "filename": "<exploit_name.py>",
        "code": "<the complete exploit code>",
        "dependencies": ["<required packages>"]
    }},
    "usage_instructions": {{
        "setup": ["<setup step 1>", "<setup step 2>"],
        "execution": "<how to run the exploit>",
        "expected_output": "<what successful exploitation looks like>"
    }},
    "payload_details": {{
        "payload": "<the actual payload used>",
        "encoding": "<any encoding applied>",
        "explanation": "<why this payload works>"
    }},
    "verification": {{
        "method": "<how to verify success>",
        "indicators": ["<success indicator 1>", "<indicator 2>"]
    }}
}}
```"""

    EXPLOIT_REFINEMENT_PROMPT = """## Task: Refine Exploit Based on Results

The exploit was tested but didn't fully succeed. Analyze the results and improve it.

---

## Original Exploit:

{original_exploit}

---

## Test Results:

{test_results}

---

## Error Analysis:

{error_analysis}

---

## Your Task

Analyze WHY the exploit failed and generate an improved version.

Consider:
1. Did the payload reach the vulnerable code?
2. Were there filters or protections we didn't account for?
3. Is the target environment different than expected?
4. Do we need a different exploitation technique?

Respond in this exact JSON format:
```json
{{
    "failure_analysis": {{
        "root_cause": "<why it failed>",
        "missed_factors": ["<factor 1>", "<factor 2>"],
        "environment_differences": ["<difference 1>"]
    }},
    "refinement_strategy": {{
        "changes_needed": ["<change 1>", "<change 2>"],
        "new_technique": "<if switching techniques>",
        "payload_modifications": "<how to modify payload>"
    }},
    "improved_exploit": {{
        "language": "<python|c|solidity|javascript>",
        "filename": "<exploit_v2.py>",
        "code": "<the improved exploit code>",
        "changes_made": ["<change 1>", "<change 2>"]
    }},
    "confidence": <0.0-1.0>,
    "should_retry": <true|false>,
    "alternative_approach": "<if should_retry is false, what else to try>"
}}
```"""

    MAX_EXPLOIT_ATTEMPTS = 5

    def __init__(self, llm: Any = None, tools: list[Any] | None = None) -> None:
        """Initialize the ExploitBuilder agent."""
        super().__init__(llm, tools)
        self._available_tools = self._detect_available_tools()

    def get_system_prompt(self) -> str:
        """Get the system prompt."""
        return self.SYSTEM_PROMPT

    def _detect_available_tools(self) -> dict[str, bool]:
        """Detect which exploitation tools are available on the system."""
        tools = {
            # Binary exploitation
            "pwntools": self._check_python_module("pwn"),
            "ropgadget": shutil.which("ROPgadget") is not None,
            "gdb": shutil.which("gdb") is not None,
            "pwndbg": self._check_gdb_plugin("pwndbg"),
            "objdump": shutil.which("objdump") is not None,
            "readelf": shutil.which("readelf") is not None,
            "checksec": shutil.which("checksec") is not None
            or self._check_python_module("pwnlib.elf"),
            # Web exploitation
            "curl": shutil.which("curl") is not None,
            "sqlmap": shutil.which("sqlmap") is not None,
            "httpie": shutil.which("http") is not None,
            # Dynamic analysis
            "frida": self._check_python_module("frida"),
            "strace": shutil.which("strace") is not None,
            "ltrace": shutil.which("ltrace") is not None,
            # Fuzzing
            "afl": shutil.which("afl-fuzz") is not None,
            "radamsa": shutil.which("radamsa") is not None,
            # General
            "python3": shutil.which("python3") is not None,
            "docker": shutil.which("docker") is not None,
        }
        return tools

    def _check_python_module(self, module_name: str) -> bool:
        """Check if a Python module is available."""
        try:
            __import__(module_name)
            return True
        except ImportError:
            return False

    def _check_gdb_plugin(self, plugin_name: str) -> bool:
        """Check if a GDB plugin is installed."""
        # This is a simplified check
        gdbinit_paths = [
            Path.home() / ".gdbinit",
            Path("/etc/gdb/gdbinit"),
        ]
        for path in gdbinit_paths:
            if path.exists():
                try:
                    content = path.read_text()
                    if plugin_name in content.lower():
                        return True
                except Exception:
                    pass
        return False

    async def execute(self, state: AgentState) -> AgentResult:
        """Execute exploit generation using LLM-driven analysis.

        The LLM is the PRIMARY decision maker for:
        1. Analyzing the vulnerability and choosing exploitation technique
        2. Generating targeted exploit code
        3. Analyzing test results and refining exploits

        Args:
            state: Current workflow state.

        Returns:
            AgentResult with generated exploits.
        """
        target_path = Path(state.target_path)
        vulnerabilities = state.confirmed_vulnerabilities
        environment = state.environment
        exploits: list[Exploit] = []
        errors: list[str] = []
        exploit_history: list[dict] = []

        if not vulnerabilities:
            return AgentResult(
                agent_type=self.agent_type,
                success=False,
                output={"exploits": [], "message": "No vulnerabilities to exploit"},
                errors=["No confirmed vulnerabilities to exploit"],
                next_agent=AgentType.REPORTER,
            )

        # Process each vulnerability
        for vuln in vulnerabilities:
            # In HITL mode, ask before exploiting each vulnerability
            if state.mode.value == "hitl" and exploits:
                return AgentResult(
                    agent_type=self.agent_type,
                    success=True,
                    output={
                        "exploits": exploits,
                        "in_progress": True,
                        "current_vuln": vuln.id,
                    },
                    requires_human_input=True,
                    human_prompt=f"Exploit generated for previous vulnerability. "
                    f"Proceed to exploit {vuln.title}? (yes/no/skip)",
                )

            try:
                exploit_result = await self._exploit_vulnerability(
                    target_path=target_path,
                    vulnerability=vuln,
                    environment=environment,
                    state=state,
                )

                if exploit_result["success"]:
                    exploits.append(exploit_result["exploit"])
                    exploit_history.append(
                        {
                            "vuln_id": vuln.id,
                            "success": True,
                            "attempts": exploit_result["attempts"],
                            "technique": exploit_result.get("technique", "unknown"),
                        }
                    )
                else:
                    errors.append(
                        f"Failed to exploit {vuln.id}: {exploit_result.get('error', 'Unknown error')}"
                    )
                    exploit_history.append(
                        {
                            "vuln_id": vuln.id,
                            "success": False,
                            "attempts": exploit_result["attempts"],
                            "error": exploit_result.get("error", ""),
                        }
                    )

            except Exception as e:
                errors.append(f"Exception exploiting {vuln.id}: {str(e)}")

        # Save exploits to files
        if exploits:
            await self._save_exploits(exploits)

        return AgentResult(
            agent_type=self.agent_type,
            success=len(exploits) > 0,
            output={
                "exploits": exploits,
                "exploit_history": exploit_history,
                "tools_used": [t for t, avail in self._available_tools.items() if avail],
            },
            errors=errors if errors else [],
            next_agent=AgentType.REPORTER,
        )

    async def _exploit_vulnerability(
        self,
        target_path: Path,
        vulnerability: Vulnerability,
        environment: EnvironmentInfo | None,
        state: AgentState,
    ) -> dict[str, Any]:
        """Exploit a single vulnerability using LLM-driven approach.

        Args:
            target_path: Path to the codebase.
            vulnerability: Vulnerability to exploit.
            environment: Target environment info.
            state: Current workflow state.

        Returns:
            Result dict with exploit or error.
        """
        attempts = 0
        current_exploit = None
        test_results = None

        # Step 1: Gather information for LLM
        vuln_info = self._format_vulnerability_info(vulnerability, target_path)
        env_info = self._format_environment_info(environment)
        binary_analysis = await self._analyze_binary_if_applicable(target_path, vulnerability)

        # Step 2: LLM plans exploitation strategy
        strategy = await self._plan_exploitation_strategy(
            vulnerability_info=vuln_info,
            environment_info=env_info,
            available_tools=self._format_available_tools(),
        )

        if not strategy or "error" in strategy:
            error_msg = (
                strategy.get("error", "Failed to plan exploitation strategy")
                if strategy
                else "Failed to plan exploitation strategy"
            )
            return {
                "success": False,
                "attempts": 0,
                "error": error_msg,
            }

        # Step 3: Iterative exploit development
        for attempt in range(1, self.MAX_EXPLOIT_ATTEMPTS + 1):
            attempts = attempt

            if attempt == 1:
                # Generate initial exploit
                exploit_data = await self._generate_exploit(
                    vulnerability_info=vuln_info,
                    strategy=strategy,
                    environment_info=env_info,
                    binary_analysis=binary_analysis,
                )
            else:
                # Refine based on previous results
                exploit_data = await self._refine_exploit(
                    original_exploit=current_exploit,
                    test_results=test_results,
                    error_analysis=self._analyze_exploit_failure(test_results),
                )

            if not exploit_data or "error" in exploit_data:
                continue

            current_exploit = exploit_data

            # Step 4: Test the exploit (if environment available)
            if environment and environment.build_successful:
                test_results = await self._test_exploit(
                    exploit_data=exploit_data,
                    environment=environment,
                    vulnerability=vulnerability,
                )

                if test_results.get("success"):
                    # Exploitation successful!
                    exploit = self._create_exploit_object(
                        vulnerability=vulnerability,
                        exploit_data=exploit_data,
                        tested=True,
                        successful=True,
                        test_output=test_results.get("output", ""),
                    )
                    return {
                        "success": True,
                        "exploit": exploit,
                        "attempts": attempts,
                        "technique": strategy.get("exploitation_strategy", {}).get(
                            "technique", "unknown"
                        ),
                    }

                # Check if LLM says we should stop trying
                if exploit_data.get("should_retry") is False:
                    break
            else:
                # No environment - create untested exploit
                exploit = self._create_exploit_object(
                    vulnerability=vulnerability,
                    exploit_data=exploit_data,
                    tested=False,
                    successful=False,
                    test_output="No environment available for testing",
                )
                return {
                    "success": True,
                    "exploit": exploit,
                    "attempts": attempts,
                    "technique": strategy.get("exploitation_strategy", {}).get(
                        "technique", "unknown"
                    ),
                }

        # Max attempts reached
        if current_exploit:
            exploit = self._create_exploit_object(
                vulnerability=vulnerability,
                exploit_data=current_exploit,
                tested=True,
                successful=False,
                test_output=test_results.get("output", "") if test_results else "",
            )
            return {
                "success": True,  # We have an exploit, just not verified working
                "exploit": exploit,
                "attempts": attempts,
                "technique": strategy.get("exploitation_strategy", {}).get("technique", "unknown"),
                "note": "Exploit generated but not verified successful",
            }

        return {
            "success": False,
            "attempts": attempts,
            "error": "Failed to generate working exploit after max attempts",
        }

    def _format_vulnerability_info(self, vuln: Vulnerability, target_path: Path) -> str:
        """Format vulnerability information for LLM.

        Args:
            vuln: Vulnerability object.
            target_path: Path to codebase.

        Returns:
            Formatted vulnerability info.
        """
        # Get extended code context
        code_context = self._get_extended_code_context(target_path, vuln)

        info = f"""### Vulnerability: {vuln.title}

**ID:** {vuln.id}
**Type:** {vuln.vuln_type.value}
**Severity:** {vuln.severity.value} (Score: {vuln.score})
**Confidence:** {vuln.confidence:.0%}

**Location:**
- File: `{vuln.file_path}`
- Line: {vuln.line_number}

**Description:**
{vuln.description or "No description provided"}

**CWE:** {vuln.cwe_id or "Not specified"}

**Code Snippet:**
```
{vuln.code_snippet or "Not available"}
```

**Extended Code Context:**
```
{code_context}
```
"""

        if vuln.data_flow:
            info += f"""
**Data Flow:**
- Source: `{vuln.data_flow.source}` at {vuln.data_flow.source_file}:{vuln.data_flow.source_line}
- Sink: `{vuln.data_flow.sink}` at {vuln.data_flow.sink_file}:{vuln.data_flow.sink_line}
- Tainted: {vuln.data_flow.tainted}
- Sanitizers: {", ".join(vuln.data_flow.sanitizers) if vuln.data_flow.sanitizers else "None detected"}
"""

        return info

    def _get_extended_code_context(
        self, target_path: Path, vuln: Vulnerability, context_lines: int = 50
    ) -> str:
        """Get extended code context around vulnerable code.

        Args:
            target_path: Path to codebase.
            vuln: Vulnerability.
            context_lines: Lines of context.

        Returns:
            Code context with line numbers.
        """
        try:
            file_path = target_path / vuln.file_path
            if not file_path.exists():
                return "File not found"

            content = file_path.read_text(encoding="utf-8", errors="ignore")
            lines = content.split("\n")

            start = max(0, vuln.line_number - context_lines - 1)
            end = min(len(lines), vuln.line_number + context_lines)

            context_lines_with_numbers = []
            for i in range(start, end):
                line_num = i + 1
                marker = " >>> " if line_num == vuln.line_number else "     "
                context_lines_with_numbers.append(f"{line_num:4d}{marker}{lines[i]}")

            return "\n".join(context_lines_with_numbers)

        except Exception as e:
            return f"Error reading file: {e}"

    def _format_environment_info(self, environment: EnvironmentInfo | None) -> str:
        """Format environment information for LLM.

        Args:
            environment: Environment info object.

        Returns:
            Formatted environment info.
        """
        if not environment:
            return "No environment information available. Exploit will be generated for manual testing."

        info = f"""### Target Environment

**Type:** {environment.env_type}
**Build Status:** {"Successful" if environment.build_successful else "Failed"}
**Build Attempts:** {environment.build_attempts}
"""

        if environment.container_id:
            info += f"**Container ID:** {environment.container_id}\n"
        if environment.connection_port:
            info += f"**Port:** {environment.connection_port}\n"
        if environment.connection_ip:
            info += f"**IP:** {environment.connection_ip}\n"
        if environment.process_id:
            info += f"**PID:** {environment.process_id}\n"
        if environment.build_errors:
            info += f"**Build Errors:** {'; '.join(environment.build_errors)}\n"

        return info

    def _format_available_tools(self) -> str:
        """Format available tools for LLM.

        Returns:
            Formatted tool list.
        """
        available = []
        unavailable = []

        tool_descriptions = {
            "pwntools": "Python exploit development framework",
            "ropgadget": "ROP gadget finder",
            "gdb": "GNU Debugger",
            "pwndbg": "GDB plugin for exploit development",
            "objdump": "Binary disassembler",
            "readelf": "ELF file analyzer",
            "checksec": "Binary security checker",
            "curl": "HTTP client",
            "sqlmap": "SQL injection automation",
            "httpie": "HTTP client",
            "frida": "Dynamic instrumentation",
            "strace": "System call tracer",
            "ltrace": "Library call tracer",
            "afl": "Coverage-guided fuzzer",
            "radamsa": "General-purpose fuzzer",
            "python3": "Python interpreter",
            "docker": "Container runtime",
        }

        for tool, is_available in self._available_tools.items():
            desc = tool_descriptions.get(tool, "")
            if is_available:
                available.append(f"- **{tool}**: {desc}")
            else:
                unavailable.append(f"- {tool}: {desc}")

        result = "### Available Tools\n"
        result += "\n".join(available) if available else "No tools detected"

        if unavailable:
            result += "\n\n### Unavailable Tools (install for better results)\n"
            result += "\n".join(unavailable[:5])  # Limit to 5

        return result

    async def _analyze_binary_if_applicable(self, target_path: Path, vuln: Vulnerability) -> str:
        """Analyze binary if this is a binary exploitation scenario.

        Args:
            target_path: Path to codebase.
            vuln: Vulnerability.

        Returns:
            Binary analysis results.
        """
        # Check if this is likely a binary exploitation scenario
        binary_vuln_types = {
            VulnerabilityType.RCE,
            VulnerabilityType.COMMAND_INJECTION,
            VulnerabilityType.LPE,
        }

        if vuln.vuln_type not in binary_vuln_types:
            return "Not a binary exploitation scenario."

        results = []

        # Look for binary files
        binary_extensions = {".elf", ".exe", ".so", ".dll", ".out", ""}
        binaries = []
        for ext in binary_extensions:
            binaries.extend(target_path.glob(f"*{ext}"))

        if not binaries:
            return "No binary files found in target."

        # Analyze first binary found
        binary = binaries[0]
        results.append(f"**Binary:** {binary.name}")

        # Run checksec if available
        if self._available_tools.get("checksec"):
            checksec_result = await self._run_command(f"checksec --file={binary}", timeout=30)
            if checksec_result["success"]:
                results.append(
                    f"\n**Security Mitigations:**\n```\n{checksec_result['output']}\n```"
                )

        # Run file command
        file_result = await self._run_command(f"file {binary}", timeout=10)
        if file_result["success"]:
            results.append(f"\n**File Type:** {file_result['output'].strip()}")

        # Check for ROP gadgets if ROPgadget available
        if self._available_tools.get("ropgadget"):
            rop_result = await self._run_command(
                f"ROPgadget --binary={binary} --depth=3 | head -50", timeout=60
            )
            if rop_result["success"]:
                results.append(f"\n**Sample ROP Gadgets:**\n```\n{rop_result['output']}\n```")

        return "\n".join(results) if results else "Binary analysis not available."

    async def _plan_exploitation_strategy(
        self,
        vulnerability_info: str,
        environment_info: str,
        available_tools: str,
    ) -> dict[str, Any] | None:
        """Use LLM to plan the exploitation strategy.

        Args:
            vulnerability_info: Formatted vulnerability info.
            environment_info: Formatted environment info.
            available_tools: Available tools list.

        Returns:
            Exploitation strategy dict or None.
        """
        prompt = self.EXPLOIT_STRATEGY_PROMPT.format(
            vulnerability_info=vulnerability_info,
            environment_info=environment_info,
            available_tools=available_tools,
        )

        try:
            response = await self.chat(prompt)
            return self._parse_llm_json_response(response)
        except Exception as e:
            return {"error": str(e)}

    async def _generate_exploit(
        self,
        vulnerability_info: str,
        strategy: dict[str, Any],
        environment_info: str,
        binary_analysis: str,
    ) -> dict[str, Any] | None:
        """Use LLM to generate exploit code.

        Args:
            vulnerability_info: Formatted vulnerability info.
            strategy: Exploitation strategy from LLM.
            environment_info: Formatted environment info.
            binary_analysis: Binary analysis results.

        Returns:
            Exploit data dict or None.
        """
        prompt = self.EXPLOIT_GENERATION_PROMPT.format(
            vulnerability_info=vulnerability_info,
            strategy=json.dumps(strategy, indent=2),
            environment_info=environment_info,
            binary_analysis=binary_analysis,
        )

        try:
            response = await self.chat(prompt)
            return self._parse_llm_json_response(response)
        except Exception as e:
            return {"error": str(e)}

    async def _refine_exploit(
        self,
        original_exploit: dict[str, Any] | None,
        test_results: dict[str, Any] | None,
        error_analysis: str,
    ) -> dict[str, Any] | None:
        """Use LLM to refine exploit based on test results.

        Args:
            original_exploit: Previous exploit attempt.
            test_results: Results from testing.
            error_analysis: Analysis of what went wrong.

        Returns:
            Refined exploit data or None.
        """
        if not original_exploit:
            return None

        prompt = self.EXPLOIT_REFINEMENT_PROMPT.format(
            original_exploit=json.dumps(original_exploit, indent=2),
            test_results=json.dumps(test_results, indent=2) if test_results else "No test results",
            error_analysis=error_analysis,
        )

        try:
            response = await self.chat(prompt)
            result = self._parse_llm_json_response(response)

            # Merge improved exploit into standard format
            if result is not None and "improved_exploit" in result:
                result["exploit_code"] = result["improved_exploit"]

            return result
        except Exception as e:
            return {"error": str(e)}

    def _analyze_exploit_failure(self, test_results: dict[str, Any] | None) -> str:
        """Analyze why an exploit failed.

        Args:
            test_results: Test results dict.

        Returns:
            Analysis string.
        """
        if not test_results:
            return "No test results available."

        analysis_parts = []

        if test_results.get("error"):
            analysis_parts.append(f"**Error:** {test_results['error']}")

        if test_results.get("output"):
            analysis_parts.append(f"**Output:** {test_results['output'][:500]}")

        if test_results.get("return_code") is not None:
            analysis_parts.append(f"**Return Code:** {test_results['return_code']}")

        if test_results.get("timeout"):
            analysis_parts.append("**Timeout:** Exploit execution timed out")

        return "\n".join(analysis_parts) if analysis_parts else "Unknown failure reason."

    async def _test_exploit(
        self,
        exploit_data: dict[str, Any],
        environment: EnvironmentInfo,
        vulnerability: Vulnerability,
    ) -> dict[str, Any]:
        """Test an exploit against the target environment.

        Args:
            exploit_data: Generated exploit data.
            environment: Target environment.
            vulnerability: Target vulnerability.

        Returns:
            Test results dict.
        """
        from mrzero.core.config import get_config

        config = get_config()
        exploit_code = exploit_data.get("exploit_code", {})
        code = exploit_code.get("code", "")
        language = exploit_code.get("language", "python")

        if not code:
            return {"success": False, "error": "No exploit code generated"}

        # Create temporary exploit file
        output_dir = config.output_dir / "exploits" / "test"
        output_dir.mkdir(parents=True, exist_ok=True)

        filename = exploit_code.get("filename", f"exploit_{vulnerability.id}.py")
        exploit_file = output_dir / filename
        exploit_file.write_text(code)

        # Determine how to run the exploit
        if language == "python":
            cmd = f"python3 {exploit_file}"

            # Add target URL if this is a web exploit and environment has port
            if environment.connection_port:
                target_url = f"http://localhost:{environment.connection_port}"
                cmd = f"TARGET_URL={target_url} {cmd}"

        elif language == "c":
            # Compile first
            binary = output_dir / f"exploit_{vulnerability.id}"
            compile_result = await self._run_command(f"gcc -o {binary} {exploit_file}", timeout=30)
            if not compile_result["success"]:
                return {
                    "success": False,
                    "error": f"Compilation failed: {compile_result['error']}",
                }
            cmd = str(binary)

        else:
            return {
                "success": False,
                "error": f"Unsupported language: {language}",
            }

        # Run the exploit
        result = await self._run_command(cmd, timeout=60)

        # Analyze results
        success_indicators = exploit_data.get("verification", {}).get("indicators", [])
        output = result.get("output", "") + result.get("error", "")

        # Check for success indicators
        exploitation_successful = False
        for indicator in success_indicators:
            if indicator.lower() in output.lower():
                exploitation_successful = True
                break

        # Also check common success patterns
        common_success_patterns = [
            "success",
            "shell",
            "pwned",
            "flag{",
            "root:",
            "uid=0",
            "admin",
            "exploited",
        ]
        for pattern in common_success_patterns:
            if pattern in output.lower():
                exploitation_successful = True
                break

        return {
            "success": exploitation_successful,
            "output": output,
            "return_code": result.get("returncode"),
            "timeout": result.get("timeout", False),
        }

    def _create_exploit_object(
        self,
        vulnerability: Vulnerability,
        exploit_data: dict[str, Any],
        tested: bool,
        successful: bool,
        test_output: str,
    ) -> Exploit:
        """Create an Exploit object from LLM-generated data.

        Args:
            vulnerability: Target vulnerability.
            exploit_data: Generated exploit data.
            tested: Whether exploit was tested.
            successful: Whether exploitation succeeded.
            test_output: Output from testing.

        Returns:
            Exploit object.
        """
        exploit_code = exploit_data.get("exploit_code", {})
        code = exploit_code.get("code", "# Exploit generation failed")
        language = exploit_code.get("language", "python")
        filename = exploit_code.get("filename", f"exploit_{vulnerability.id}.py")

        # Build description from exploit data
        description_parts = []

        if exploit_data.get("exploitation_strategy"):
            strategy = exploit_data["exploitation_strategy"]
            description_parts.append(f"Technique: {strategy.get('technique', 'Unknown')}")
            description_parts.append(f"Goal: {strategy.get('goal', 'Unknown')}")

        if exploit_data.get("payload_details"):
            payload = exploit_data["payload_details"]
            description_parts.append(f"Payload: {payload.get('explanation', 'N/A')[:200]}")

        if exploit_data.get("usage_instructions"):
            instructions = exploit_data["usage_instructions"]
            description_parts.append(
                f"Execution: {instructions.get('execution', 'Run the script')}"
            )

        description = "\n".join(description_parts) if description_parts else None

        # Determine exploit type
        exploit_type = "poc"
        if exploit_data.get("exploitation_strategy", {}).get("goal") == "code_execution":
            exploit_type = "rce"
        elif "rop" in code.lower() or "gadget" in code.lower():
            exploit_type = "rop_chain"
        elif "reverse_shell" in code.lower() or "shell" in code.lower():
            exploit_type = "reverse_shell"

        notes = []
        if exploit_data.get("failure_analysis"):
            notes.append(f"Refinement: {exploit_data['failure_analysis'].get('root_cause', '')}")
        if not successful and tested:
            notes.append(f"Testing output: {test_output[:200]}")

        return Exploit(
            vulnerability_id=vulnerability.id,
            exploit_type=exploit_type,
            language=language,
            code=code,
            description=description,
            file_path=None,  # Will be set when saved
            tested=tested,
            successful=successful,
            test_output=test_output[:500] if test_output else None,
            notes="\n".join(notes) if notes else None,
        )

    async def _save_exploits(self, exploits: list[Exploit]) -> None:
        """Save generated exploits to files.

        Args:
            exploits: List of exploits to save.
        """
        from mrzero.core.config import get_config

        config = get_config()
        output_dir = config.output_dir / "exploits"
        output_dir.mkdir(parents=True, exist_ok=True)

        for exploit in exploits:
            # Determine file extension
            ext_map = {
                "python": ".py",
                "c": ".c",
                "solidity": ".sol",
                "javascript": ".js",
            }
            ext = ext_map.get(exploit.language, ".txt")

            # Generate filename
            filename = f"exploit_{exploit.vulnerability_id}{ext}"
            exploit_file = output_dir / filename

            # Write exploit code
            exploit_file.write_text(exploit.code)
            exploit.file_path = str(exploit_file)

            # Also write a companion info file
            info_file = output_dir / f"exploit_{exploit.vulnerability_id}_info.md"
            info_content = f"""# Exploit Information

**Vulnerability ID:** {exploit.vulnerability_id}
**Exploit Type:** {exploit.exploit_type}
**Language:** {exploit.language}
**Tested:** {"Yes" if exploit.tested else "No"}
**Successful:** {"Yes" if exploit.successful else "No"}

## Description

{exploit.description or "No description available."}

## Notes

{exploit.notes or "No notes."}

## Test Output

```
{exploit.test_output or "No test output."}
```
"""
            info_file.write_text(info_content)

    async def _run_command(self, command: str, timeout: int = 60) -> dict[str, Any]:
        """Run a shell command.

        Args:
            command: Command to run.
            timeout: Timeout in seconds.

        Returns:
            Result dict with output, error, returncode.
        """
        try:
            proc = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )

            stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=timeout)

            return {
                "success": proc.returncode == 0,
                "output": stdout.decode("utf-8", errors="ignore"),
                "error": stderr.decode("utf-8", errors="ignore"),
                "returncode": proc.returncode,
                "timeout": False,
            }

        except asyncio.TimeoutError:
            return {
                "success": False,
                "output": "",
                "error": f"Command timed out after {timeout}s",
                "returncode": -1,
                "timeout": True,
            }
        except Exception as e:
            return {
                "success": False,
                "output": "",
                "error": str(e),
                "returncode": -1,
                "timeout": False,
            }

    def _parse_llm_json_response(self, response: str) -> dict[str, Any] | None:
        """Parse LLM's JSON response.

        Args:
            response: Raw LLM response.

        Returns:
            Parsed JSON dict or None.
        """
        try:
            # Try to find JSON in ```json ... ``` blocks
            json_block_match = re.search(r"```json\s*([\s\S]*?)\s*```", response)
            if json_block_match:
                return json.loads(json_block_match.group(1))

            # Try to find raw JSON object
            json_match = re.search(r"\{[\s\S]*\}", response)
            if json_match:
                return json.loads(json_match.group())

            return None

        except json.JSONDecodeError:
            return None
